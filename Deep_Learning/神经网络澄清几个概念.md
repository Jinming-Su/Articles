* epoch: 1个epoch就是所有训练样例全部训练一次
* batch-size(mini-batch): 由于把所有的样例进行一次训练，然后求偏导数计算量太大，所以采用随机梯度下降算法SGD.SGD每次采样batch-size个样例进行训练，然后进行一次Forward和BP操作.
* iterations: 每一次迭代就是一次权重更新，也就是一个mini-batch训练完成.
* 因此，有**总的训练样本数是N,则一个epoch训练的样本数就是N,一个batch-size大小为n，则iterations = N/n**
