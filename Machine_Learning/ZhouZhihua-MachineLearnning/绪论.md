---
title: 绪论
date: 2017-03-10 00:00:01
categories:
- Machine_Learning
- ZhouZhihua
tags:
- machine_learning
description: 关于周志华《机器学习》学习过程中的笔记
---


### 基本概念
**机器学习**: [Mitchell, 1997]: 假设用P来评估计算机程序在某任务类T上的性能，若一个程序通过利用经验E在T中任务上获得了性能改善，则我们就说关于T和P，该程序对E进行了学习.  
**独立同分布**: '独立'指随机变量x1与x2互不影响，'同分布'指随机变量x1和x2具有相同的分布形状和相同的分布参数.  
**归纳偏好**: 指机器学习算法在学习过程中对某种类型假设的偏好.例如，有的算法偏好“尽可能特殊”模型，有的算法则偏好“尽可能一般”模型。注释，偏好对算法很重要，没有偏好可能导致一个算法被“等效”假设所迷惑.  
**奥卡姆剃刀(Occam's rezor)原则**:若有多个假设与观察一致，则选择最简单的.  
**没有免费午餐定理(No Free Lunch, NRL)**: 由于对所有的可能的函数的相互补偿，最优化算法的性能是等价的.NFL的一个重要前提是: 所有“问题”出现的可能性相同、或所有问题同等重要.但是实际中，我们通常只关注正在解决的问题，对于这个解决方案在其他情况下的表现并不关注，所以这个定理对于我们暂时并没有实际意义.<font color="red"><b>NFL定理最重要的寓意，是让我们清楚的认识到，脱离具体问题空谈“什么学习算法最好”毫无意义，因为若考虑所有存在的问题，在所有算法一样好.</b></font>  
这个定理的简单证明可以参考《机器学习》的8-9页.对于其中一句'考虑二分类问题，且真实目标函数可以是任何函数X->{0, 1}, 函数空间为{0, 1}^|X|'这句话的理解:函数的映射f: X->{0, 1}, 样本空间大小为|X|，选择其中的样本进行训练，对于一个样本，可能选择也可能不选择，所以选择的样本情况数是 {0, 1}^|X|,也就是函数的可能性数目，也就是函数空间.  
**丑小鸭定理**: 丑小鸭和白天鹅之间的区别和两只白天鹅之间的区别一样大.实质是说: 世界上不存在分类的客观性，一切分类的标准都是主观的.

### 相关介绍
**Tom Mitchell**: 曾写过一本著作《Machine Learning》,《Machine Learning》这本书是第一本机器学习的专门性教材，他现在是CMU的professor. CMU的Mitchell Home Page: http://www.cs.cmu.edu/~tom/. 

**一些重要的会议**  
* 机器学习领域会议
    * 国际机器学习会议ICML
    * 国际神经信息处理系统会议NIPS
    * 国际学习理论会议COLT
    * 欧洲机器学习会议ECML
    * 亚洲机器学习会议ACML
* 机器学习领域期刊
    * *Journal of Machine Learning*
    * *Machine Learning*
* 人工智能领域会议
    * IJCAI
    * AAAI
* 人工智能领域期刊
    * *Artificial Intelligence*
    * *Journal of Artificial Intelligence Research*
* 数据挖掘领域会议
    * KDD
    * ICDM
* 数据挖掘领域期刊
    * *ACM Transactions on Knowledge Deiscovery from Data*
    * *Data Mining and Knowledge Discovery*
* CV & PR会议
    * CVPR, ICCV, ECCV
* CV & PR期刊
    * *IEEE Transactions on Pattern Analysis and Machine Intelligence*
* NN期刊
    * *Neural Computation*
    * *IEEE Transactions on Neural Networks and Learning Systems*
